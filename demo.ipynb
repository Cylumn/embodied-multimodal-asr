{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da977075",
   "metadata": {},
   "source": [
    "# [ðŸ“„ Multimodal Speech Recognition for Language-Guided Embodied Agents](https://arxiv.org/abs/2302.14030)\n",
    "[Allen Chang](https://www.cylumn.com/), \n",
    "[Xiaoyuan Zhu](https://www.linkedin.com/in/xiaoyuan-zhu-38005a224/), \n",
    "[Aarav Monga](https://www.linkedin.com/in/aarav-monga-517457246/), \n",
    "[Seoho Ahn](https://www.linkedin.com/in/sean-ahn-437423220/),\n",
    "[Tejas Srinivasan](https://tejas1995.github.io/), \n",
    "[Jesse Thomason](https://jessethomason.com/)\n",
    "\n",
    "## Colab Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ac61d",
   "metadata": {},
   "source": [
    "### Install dependencies and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "if not exists('embodied-multimodal-asr'):\n",
    "  !git clone -q --depth 1 https://github.com/Cylumn/embodied-multimodal-asr\n",
    "\n",
    "%cd embodied-multimodal-asr\n",
    "\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install torch==1.12.0\n",
    "!pip install torchaudio==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920839cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd models\n",
    "!sh download_pretrained.sh\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b57617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import IPython\n",
    "\n",
    "from lib.models import UnimodalDecoder, MultimodalDecoder, ASRPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab505f",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b53b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenizer\n",
    "tokenizer = LabelEncoder()\n",
    "tokenizer.classes_ = np.load('media/demo/tokenizer.npy')\n",
    "n_tokens = len(tokenizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a164635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ASR Models\n",
    "unimodal = ASRPipeline(\n",
    "    decoder=UnimodalDecoder(\n",
    "        d_audio=[312, 768], d_out=n_tokens, \n",
    "        depth=4, max_target_len=25, dropout=0.3\n",
    "    ),\n",
    "    tokenizer=tokenizer, device=device\n",
    ")\n",
    "multimodal = ASRPipeline(\n",
    "    decoder=MultimodalDecoder(\n",
    "        d_audio=[312, 768], d_vision=512, d_out=n_tokens, \n",
    "        depth=4, max_target_len=25, dropout=0.3\n",
    "    ),\n",
    "    tokenizer=tokenizer, device=device\n",
    ")\n",
    "unimodal.eval()\n",
    "multimodal.eval()\n",
    "\n",
    "def load_weights():\n",
    "    unimodal.decoder.load_state_dict(\n",
    "        torch.load(f'models/unimodal_[{speaker_label}_{noise}]_pretrained.pt', map_location=device)\n",
    "    )\n",
    "    multimodal.decoder.load_state_dict(\n",
    "        torch.load(f'models/multimodal_[{speaker_label}_{noise}]_pretrained.pt', map_location=device)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faed0a8",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Try out different input permutations, and see what the model predicts!\n",
    "\n",
    "Here is a great combination to try. Toggle values indicated by ðŸ”ƒ:\\\n",
    "`\"seen\", \"unheard\", \"indic\", \"mask_0.4_nouns\", ðŸ”ƒ {knife/lettuce}.wav, ðŸ”ƒ {knife/lettuce}.jpeg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform_path():\n",
    "    return f\"media/demo/test_{seen_env}_{heard_speaker}/{speaker_label}_{noise}/{waveform}\"\n",
    "def get_image_path():\n",
    "    return f\"media/demo/test_{seen_env}_{heard_speaker}/{image}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2535d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown { run: \"auto\" }\n",
    "seen_env = \"seen\"              #@param [\"seen\", \"unseen\"]\n",
    "heard_speaker = \"unheard\"      #@param [\"heard\", \"unheard\"]\n",
    "speaker_label = \"indic\"        #@param [\"american\", \"indic\"]\n",
    "noise = \"mask_0.4_nouns\"       #@param [\"clean\", \"mask_0.4_nouns\"]\n",
    "waveform = \"knife.wav\"         #@param [\"knife.wav\", \"lettuce.wav\"]\n",
    "image = \"knife.jpeg\"           #@param [\"knife.jpeg\", \"lettuce.jpeg\"]\n",
    "\n",
    "if heard_speaker == \"unheard\":\n",
    "    assert speaker_label != \"american\", \"Unheard tests only apply to Indic English TTS Speakers\"\n",
    "if waveform == \"knife.wav\":\n",
    "    text_instruction = \"Pick up the knife on the counter.\"\n",
    "elif waveform == \"lettuce.wav\":\n",
    "    text_instruction = \"Pick up the lettuce on the counter.\"\n",
    "\n",
    "audio = torchaudio.load(get_waveform_path())[0]\n",
    "vision = Image.open(get_image_path())\n",
    "load_weights()\n",
    "\n",
    "display(vision)\n",
    "display(IPython.display.Audio(get_waveform_path()))\n",
    "\n",
    "print(f'Ground-Truth Instruction Text: \"{text_instruction}\"')\n",
    "print(f'Unimodal ASR Transcript: \"{unimodal(audio)}\"')\n",
    "print(f'Multimodal ASR Transcript: \"{multimodal(audio, vision)}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_exps",
   "language": "python",
   "name": "asr_exps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
